{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chong\\Desktop\\Coding\\GitHub\\The-Orientator-PW-2023\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from datasets import (\n",
    "    Dataset, \n",
    "    load_metric,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, \n",
    "    TrainingArguments, Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.utils import logging\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m logging\u001b[39m.\u001b[39mset_verbosity_error()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "PARENT_DIR = os.environ.get(\"PARENT_DIR\")\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model... \", end='', flush=True)\n",
    "tokeniser = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model.to(device)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/chong/Desktop/Coding/GitHub/The-Orientator-PW-2023/src/data/augmented_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         question\u001b[39m.\u001b[39mappend(q)\n\u001b[0;32m     12\u001b[0m         answer\u001b[39m.\u001b[39mappend(a)\n\u001b[1;32m---> 14\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(PARENT_DIR \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msrc/data/augmented_data.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     15\u001b[0m     csvreader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(file)\n\u001b[0;32m     16\u001b[0m     val \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m v: v, csvreader)][\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\chong\\Desktop\\Coding\\GitHub\\The-Orientator-PW-2023\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/chong/Desktop/Coding/GitHub/The-Orientator-PW-2023/src/data/augmented_data.csv'"
     ]
    }
   ],
   "source": [
    "# raw_dataset = load_dataset(\"vicgalle/alpaca-gpt4\", split=\"train\")\n",
    "# raw_dataset = raw_dataset.shuffle(seed=42).select(range(10))\n",
    "\n",
    "raw_dataset = None\n",
    "question = []\n",
    "answer = []\n",
    "\n",
    "with open(PARENT_DIR + \"src/data/base_data.csv\", 'r') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    for q, a in list(csvreader)[1:]:\n",
    "        question.append(q)\n",
    "        answer.append(a)\n",
    "\n",
    "with open(PARENT_DIR + \"src/data/augmented_data.csv\", 'r') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    val = [*filter(lambda v: v, csvreader)][1:]\n",
    "    for q, a in val:\n",
    "        question.append(q)\n",
    "        answer.append(a)\n",
    "\n",
    "question = question[:5]\n",
    "answer = answer[:5]\n",
    "\n",
    "raw_dataset = Dataset.from_dict({\"Question\" : question, \"Answer\" : answer})\n",
    "\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_lst = []\n",
    "\n",
    "for row in tqdm.tqdm(raw_dataset, desc=\"Re-formatting dataset\", unit=\" rows\"):\n",
    "    temp_dict = {}\n",
    "    temp_dict[\"text\"] = row[\"Question\"].strip() + tokeniser.eos_token + row[\"Answer\"].strip() + tokeniser.eos_token\n",
    "    temp_lst.append(temp_dict)\n",
    "\n",
    "temp_df = pd.DataFrame(temp_lst, columns=[\"text\"])\n",
    "temp_df.dropna()\n",
    "\n",
    "processed_dataset = Dataset.from_pandas(temp_df)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser.pad_token = tokeniser.eos_token\n",
    "\n",
    "def preprocess(example):\n",
    "    return tokeniser(example[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenised_dataset = processed_dataset.map(preprocess)\n",
    "\n",
    "tokenised_dataset = tokenised_dataset.remove_columns([\"text\"])\n",
    "tokenised_dataset = tokenised_dataset.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "tokenised_dataset = tokenised_dataset.train_test_split(test_size=0.1)\n",
    "tokenised_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Data Collator...\", end=\"\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokeniser, mlm=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"model-\" + datetime.datetime.now().strftime(\"%H%M%S\")\n",
    "print(\"Model will be saved as :\", f\"`{model_id}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    do_predict = True,\n",
    "    output_dir = PARENT_DIR + f\"models/{model_id}\",\n",
    "    overwrite_output_dir = True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    save_on_each_node = True,\n",
    "    optim = \"adamw_torch\",\n",
    "    report_to = \"all\",\n",
    "    load_best_model_at_end = True,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_dataset[\"train\"],\n",
    "    eval_dataset=tokenised_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokeniser,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = trainer.train()\n",
    "data = {\n",
    "    \"global_step\": train_output.global_step,\n",
    "    \"training_loss\": train_output.training_loss,\n",
    "    \"metrics\": train_output.metrics,\n",
    "}\n",
    "\n",
    "data[\"metrics\"] = [(key, value) for key, value in train_output.metrics.items()]\n",
    "\n",
    "print(\"== Training Completed ==\")\n",
    "for i, k, v in enumerate(data.items()):\n",
    "    if i < 2:\n",
    "        print(\" \".join([j.capitalize() for j in k.split(\"_\")]), \":\", round(v, 5))\n",
    "    else:\n",
    "        print(\" \".join([j.capitalize() for j in k.split(\"_\")]), \":\")\n",
    "        for f, s in v.items():\n",
    "            print(\"\\tâ€¢\", \" \".join([j.replace(\"eval\", \"evaluation\").capitalize() for j in s.split(\"_\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving model...\", end=\"\")\n",
    "trainer.save_model(PARENT_DIR + f\"models/{model_id}/final\")  \n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
